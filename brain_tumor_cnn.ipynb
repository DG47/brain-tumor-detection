{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Augmentation\n",
    "\n",
    "The dataset we’re using is relatively small, containing only 253 images divided into two categories:\n",
    "- **yes:** Images with brain tumor\n",
    "- **no:** Images without brain tumor\n",
    "\n",
    "To improve the performance of our Convolutional Neural Network (CNN) model and prevent overfitting, we will employ **data augmentation** techniques. Data augmentation artificially increases the size of our training dataset by applying various transformations to the existing images. This approach helps the model generalize better by learning from a more diverse set of training examples.\n",
    "\n",
    "The following augmentation techniques are used:\n",
    "\n",
    "- **Rotation:** ±10 degrees\n",
    "- **Width Shift:** ±10% horizontally\n",
    "- **Weight Shift:** ±10% vertically\n",
    "- **Shear Transformation:** Shearing up to 10%\n",
    "- **Brightness Adjustment:** Scaling brightness between 30% and 100%\n",
    "- **Horizontal Flips and Vertical Flips:** Random flipping of images\n",
    "- **Fill Mode:** Filling empty areas created by transformations with nearest pixel values\n",
    "\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T00:01:42.927147Z",
     "start_time": "2024-10-30T00:01:42.924488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks, models, preprocessing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def augment_data(file_dir, n_generated_samples, save_to_dir):\n",
    "    \"\"\"\n",
    "    Augments images in the specified directory and saves them to a target directory.\n",
    "\n",
    "    Parameters:\n",
    "        file_dir (str): Directory containing the original images.\n",
    "        n_generated_samples (int): Number of augmented images to generate per original image.\n",
    "        save_to_dir (str): Directory to save the augmented images.\n",
    "    \"\"\"\n",
    "    # Define the data augmentation parameters\n",
    "    data_gen = ImageDataGenerator(\n",
    "        rotation_range=10,          # Random rotation between -10 and 10 degrees\n",
    "        width_shift_range=0.1,      # Random horizontal shift\n",
    "        height_shift_range=0.1,     # Random vertical shift\n",
    "        shear_range=0.1,            # Shear transformations\n",
    "        brightness_range=(0.3, 1.0),# Random brightness adjustments\n",
    "        horizontal_flip=True,       # Randomly flip images horizontally\n",
    "        vertical_flip=True,         # Randomly flip images vertically\n",
    "        fill_mode='nearest'         # Fill mode for newly created pixels\n",
    "    )\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_to_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(file_dir):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(file_dir, filename)\n",
    "        \n",
    "        # Load the image\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Couldn't load image {file_path}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Reshape the image to add an extra dimension (batch size)\n",
    "        image = image.reshape((1,) + image.shape)\n",
    "        \n",
    "        # Create a prefix for the augmented images\n",
    "        save_prefix = 'aug_' + os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Generate augmented images\n",
    "        total = 0\n",
    "        for batch in data_gen.flow(\n",
    "            x=image,\n",
    "            batch_size=1,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format='jpg'\n",
    "        ):\n",
    "            total += 1\n",
    "            if total >= n_generated_samples:\n",
    "                break"
   ],
   "id": "6270965f0f7f702e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our dataset is imbalanced, containing approximately **61%** non-tumorous images and 39% tumorous images. An imbalanced dataset can lead to a biased model that performs poorly on the minority class (in this case, tumorous images). To address this issue and improve the model’s ability to detect brain tumors, we need to balance the dataset.\n",
    "\n",
    "To achieve a balanced dataset, we will use data augmentation to increase the number of tumorous images. Specifically, for every **9 non-tumorous images**, we will generate **6 augmented tumorous images**. This strategy ensures that both classes have a comparable number of samples, which helps the model learn equally from both categories and improves overall performance.\n",
    "\n",
    "By carefully augmenting the tumorous images, we enhance the diversity and representation of the minority class without excessively oversampling it. This balanced approach contributes to building a more robust and unbiased Convolutional Neural Network (CNN) model for brain tumor detection.\n",
    "\n",
    "***Note:** The data augmentation techniques applied to the tumorous images include rotations, shifts, shearing, brightness adjustments, and flips. These transformations create varied versions of the original images, enriching the dataset and aiding the model in generalizing better to unseen data.*"
   ],
   "id": "3673709583efc904"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define base directory\n",
    "base_dir = '/content/drive/MyDrive'\n",
    "\n",
    "# Define paths\n",
    "augmented_data_dir = os.path.join(base_dir, 'augmented_data')\n",
    "yes_dir = os.path.join(base_dir, 'yes')\n",
    "no_dir = os.path.join(base_dir, 'no')\n",
    "\n",
    "augmented_yes_dir = os.path.join(augmented_data_dir, 'yes')\n",
    "augmented_no_dir = os.path.join(augmented_data_dir, 'no')\n",
    "\n",
    "# Ensure augmented data directories exist\n",
    "os.makedirs(augmented_yes_dir, exist_ok=True)\n",
    "os.makedirs(augmented_no_dir, exist_ok=True)\n",
    "\n",
    "# Augment data for tumorous examples ('yes' label)\n",
    "augment_data(\n",
    "    file_dir=yes_dir,\n",
    "    n_generated_samples=6,\n",
    "    save_to_dir=augmented_yes_dir\n",
    ")\n",
    "\n",
    "# Augment data for non-tumorous examples ('no' label)\n",
    "augment_data(\n",
    "    file_dir=no_dir,\n",
    "    n_generated_samples=9,\n",
    "    save_to_dir=augmented_no_dir\n",
    ")"
   ],
   "id": "ae6ec03cfea45fc8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Examining the Dataset After Augmentation\n",
    "\n",
    "After performing data augmentation, let’s examine the number of tumorous and non-tumorous examples now present in our dataset:\n",
    "- **Tumorous Examples:** The total number of images labeled as “yes” (tumorous).\n",
    "- **Tumorous Examples:** The total number of images labeled as “no” (non-tumorous).\n",
    "\n",
    "By comparing these counts, we can verify that our dataset is balanced and ready for training our Convolutional Neural Network (CNN) model.\n",
    "\n",
    "***Note:** It’s important to ensure that the dataset is balanced after augmentation to prevent the model from becoming biased toward one class. A balanced dataset helps the model learn equally from both classes, improving its ability to accurately classify new, unseen images.*"
   ],
   "id": "42c253992e33afe4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the augmented data directory\n",
    "augmented_data_dir = '/content/drive/MyDrive/augmented_data/'\n",
    "\n",
    "# Define paths for tumorous ('yes') and non-tumorous ('no') images\n",
    "yes_dir = os.path.join(augmented_data_dir, 'yes')\n",
    "no_dir = os.path.join(augmented_data_dir, 'no')\n",
    "\n",
    "# Count the number of images in each category\n",
    "num_tumorous = len(os.listdir(yes_dir))\n",
    "num_non_tumorous = len(os.listdir(no_dir))\n",
    "\n",
    "# Calculate total number of images\n",
    "total_images = num_tumorous + num_non_tumorous\n",
    "\n",
    "# Calculate percentages\n",
    "percent_tumorous = (num_tumorous / total_images) * 100\n",
    "percent_non_tumorous = (num_non_tumorous / total_images) * 100\n",
    "\n",
    "# Display the results\n",
    "print(\"Percentage of tumorous examples: {:.2f}%\".format(percent_tumorous))\n",
    "print(\"Percentage of non-tumorous examples: {:.2f}%\".format(percent_non_tumorous))"
   ],
   "id": "3e66f91aed0f2057"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Prepossessing and Preparation\n",
    "\n",
    "\n",
    "Before training our neural network, it’s crucial to preprocess the data to enhance the model’s performance. One effective preprocessing step is to crop the images to focus exclusively on the brain region, eliminating any irrelevant background and noise.\n",
    "\n",
    "We will accomplish this using the *crop_image()* function. This function detects the extreme points in the contours of the brain within each image and crops the image accordingly to isolate the area of interest.\n",
    "\n",
    "\n",
    "**Reference:** [Finding Extreme Points in Contours using OpenCV](https://pyimagesearch.com/2016/04/11/finding-extreme-points-in-contours-with-opencv/)\n",
    "\n",
    "\n",
    "By cropping the images to include only the brain, we:\n",
    "\n",
    "\n",
    "- **Reduce Computational Load:** Smaller images mean less data for the network to process, speeding up training.\n",
    "- **Improve Accuracy:** Eliminating irrelevant features helps the model focus on the essential patterns associated with brain tumors.\n",
    "- **Enhance Generalization:** Reducing noise and variability in the input data leads to better performance on unseen data.\n",
    "\n",
    "\n",
    "***Note:** Ensure that the crop_image() function is correctly implemented and tested. It should handle variations in image brightness, contrast, and noise to reliably detect the brain contours in all images.*"
   ],
   "id": "c39fb3c20c6c2040"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:49:35.020294Z",
     "start_time": "2024-10-29T22:49:35.012812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def crop_image(image):\n",
    "    \"\"\"\n",
    "    Crops the input image to focus on the brain region by finding the extreme points of the largest contour.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): The input image in BGR format.\n",
    "\n",
    "    Returns:\n",
    "        new_image (numpy.ndarray): The cropped image containing only the brain region.\n",
    "    \"\"\"\n",
    "    # Convert the image to grayscale and apply Gaussian blur\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Apply thresholding to create a binary image\n",
    "    _, thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Perform erosions and dilations to remove small noise\n",
    "    thresh = cv2.erode(thresh, None, iterations=2)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "    # Find contours in the thresholded image\n",
    "    cnts = cv2.findContours(\n",
    "        thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "\n",
    "    # Handle compatibility between OpenCV versions\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "    if not cnts:\n",
    "        print(\"Warning: No contours found. Returning the original image.\")\n",
    "        return image\n",
    "\n",
    "    # Find the largest contour\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "    # Determine the extreme points\n",
    "    ext_left = tuple(c[c[:, :, 0].argmin()][0])\n",
    "    ext_right = tuple(c[c[:, :, 0].argmax()][0])\n",
    "    ext_top = tuple(c[c[:, :, 1].argmin()][0])\n",
    "    ext_bottom = tuple(c[c[:, :, 1].argmax()][0])\n",
    "\n",
    "    # Crop the image using the extreme points\n",
    "    new_image = image[ext_top[1]:ext_bottom[1], ext_left[0]:ext_right[0]]\n",
    "\n",
    "    return new_image"
   ],
   "id": "ebee8da34433fde5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We are now ready to load our data and prepare it for training our neural network. We will process the images from the augmented 'yes' and 'no' folders using the following steps:\n",
    "\n",
    "1.\t**Read the Image:** Load each image file into memory.\n",
    "2.  **Crop the Image:** Apply the crop_image() function to extract only the brain region, removing unnecessary background.\n",
    "3.\t**Resize the Image:** Resize the cropped image to a uniform size of **(240, 240, 3)** to ensure consistency across all input data.\n",
    "4.\t**Normalize Pixel Values:** Scale the pixel values to lie within the range **0 to 1** by dividing by 255. This normalization helps the neural network train more effectively.\n",
    "5.\t**Append to Data Arrays:**\n",
    "\t-   **Features (X):** Add the processed image to the features array X.\n",
    "\t-   **Labels (y):** Add the corresponding label to the labels array y:\n",
    "\t\t-   1 for tumorous images ('yes' folder).\n",
    "\t\t-   0 for non-tumorous images ('no' folder).\n",
    "\n",
    "By following these preprocessing steps, we ensure that our dataset is properly formatted and ready for input into our Convolutional Neural Network (CNN) model. This preparation is crucial for the model to learn effectively from the data and make accurate predictions.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "-\t**Split the Data:** After preprocessing, we’ll split the dataset into training and testing sets to evaluate the model’s performance.\n",
    "-\t**Model Training:** Use the prepared data to train the CNN, adjusting hyperparameters as needed.\n",
    "-\t**Evaluation:** Assess the model’s accuracy, precision, recall, and F1-score to determine its effectiveness in detecting brain tumors.\n",
    "\n",
    "By systematically loading, preprocessing, and preparing our data, we set a strong foundation for building an effective brain tumor detection model."
   ],
   "id": "d3d3235a67bb2fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize empty lists for features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Set image dimensions\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 240, 240\n",
    "\n",
    "# Paths to the augmented data directories\n",
    "augmented_data_dir = '/content/drive/MyDrive/augmented_data'\n",
    "yes_dir = os.path.join(augmented_data_dir, 'yes')\n",
    "no_dir = os.path.join(augmented_data_dir, 'no')\n",
    "\n",
    "# List of directories with their corresponding labels\n",
    "directories = [\n",
    "    (yes_dir, 1),  # Tumorous images labeled as 1\n",
    "    (no_dir, 0)    # Non-tumorous images labeled as 0\n",
    "]\n",
    "\n",
    "# Process each image in the directories\n",
    "for directory, label in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the image\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Unable to read image {file_path}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Crop the image to include only the brain region\n",
    "        image = crop_image(image)\n",
    "        if image is None or image.size == 0:\n",
    "            print(f\"Warning: Cropped image is empty for {file_path}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Resize the image to a uniform size\n",
    "        image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Normalize the pixel values\n",
    "        image = image / 255.0\n",
    "        \n",
    "        # Append the image and its label to the lists\n",
    "        X.append(image)\n",
    "        y.append(label)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Shuffle the dataset\n",
    "X, y = shuffle(X, y, random_state=42)"
   ],
   "id": "f8871875901605fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Plotting Sample Images\n",
    "\n",
    "After preprocessing and preparing our dataset, it’s important to visualize some sample images to verify that the preprocessing steps have been applied correctly. Plotting sample images from both classes (tumorous and non-tumorous) helps us ensure that:\n",
    "\n",
    "-\tThe images are correctly cropped to focus on the brain region.\n",
    "-\tThe images are resized to the expected dimensions.\n",
    "-\tThe normalization is applied correctly.\n",
    "-\tThe labels correspond to the correct images.\n"
   ],
   "id": "5da4ef55e121e09d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_sample_images(X, y, n=30):\n",
    "    \"\"\"\n",
    "    Plots sample images from the dataset for each class label.\n",
    "\n",
    "    Parameters:\n",
    "        X (numpy.ndarray): Array of images.\n",
    "        y (numpy.ndarray): Array of labels corresponding to images in X.\n",
    "        n (int): Number of images to display per class (default is 30).\n",
    "    \"\"\"\n",
    "    label_names = {0: \"No\", 1: \"Yes\"}\n",
    "\n",
    "    for label in [0, 1]:\n",
    "        # Get indices of images with the current label\n",
    "        indices = np.where(y == label)[0]\n",
    "\n",
    "        # If there are fewer images than n, adjust n\n",
    "        n_images = min(n, len(indices))\n",
    "        if n_images == 0:\n",
    "            print(f\"No images found for label {label}.\")\n",
    "            continue\n",
    "\n",
    "        # Randomly select n_images indices\n",
    "        selected_indices = np.random.choice(indices, size=n_images, replace=False)\n",
    "\n",
    "        # Calculate the number of rows and columns for the subplot grid\n",
    "        columns = 10\n",
    "        rows = int(np.ceil(n_images / columns))\n",
    "\n",
    "        plt.figure(figsize=(20, 2 * rows))\n",
    "\n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            plt.subplot(rows, columns, i + 1)\n",
    "            plt.imshow(X[idx])\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Brain Tumor: {label_names[label]}\", fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_sample_images(X, y)"
   ],
   "id": "54ac8292935fadee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Splitting the Dataset",
   "id": "2decc2065edfcebd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split the dataset into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Display the sizes of the training and validation sets\n",
    "print(f'Training set size: {X_train.shape}')\n",
    "print(f'Validation set size: {X_valid.shape}')"
   ],
   "id": "74ea7a9e20e9c704"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building the Model\n",
    "\n",
    "We are constructing a Convolutional Neural Network (CNN) to perform the classification task of detecting brain tumors. The architecture is designed to be straightforward yet effective, consisting of the following components:\n",
    "\n",
    "1.\t**Convolutional Blocks:**\n",
    "\t-\t**Two Consecutive Layers:** Each block includes:\n",
    "\t-\t**Convolutional Layer:** Applies a set of filters to extract relevant features from the input images.\n",
    "\t-\t**Batch Normalization:** Normalizes the outputs of the convolutional layer to accelerate training and improve stability.\n",
    "\t-\t**Activation Function:** Uses a nonlinear activation function (e.g., ReLU) to introduce nonlinearity.\n",
    "\t-\t**Max Pooling:** Reduces the spatial dimensions of the feature maps, which helps in reducing the computational load and controlling overfitting.\n",
    "2.\t**Flattening Layer:**\n",
    "\t-\tTransforms the pooled feature maps into a one-dimensional vector to serve as input for the fully connected layer.\n",
    "3.\t**Fully Connected Layer:**\n",
    "\t-\tProcesses the flattened features to learn complex patterns and relationships.\n",
    "4.\t**Output Layer:**\n",
    "\t-\t**Single Neuron with Sigmoid Activation:** Produces a probability between 0 and 1, suitable for binary classification tasks.\n",
    "\n",
    "This architecture strikes a balance between simplicity and performance, making it well-suited for our goal of classifying images as tumorous or non-tumorous. By stacking convolutional layers with batch normalization and max pooling, we enable the model to learn hierarchical feature representations, improving its ability to detect subtle differences in brain images.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "-\t**Compile the Model:** Define the optimizer, loss function, and evaluation metrics.\n",
    "-\t**Train the Model:** Fit the model on the training data and validate it using the validation set.\n",
    "-\t**Evaluate Performance:** Analyze the model’s performance using appropriate metrics like accuracy, precision, recall, and F1-score.\n",
    "-\t**Fine-tuning:** Adjust hyperparameters or modify the architecture as needed based on evaluation results.\n",
    "\n",
    "By carefully designing the CNN architecture, we aim to achieve high accuracy in detecting brain tumors, contributing valuable insights to medical diagnostics."
   ],
   "id": "e22f55b5ca8ad090"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define constants\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 240, 240\n",
    "INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n",
    "\n",
    "# Build the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=INPUT_SHAPE),\n",
    "    \n",
    "    keras.layers.ZeroPadding2D(padding=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(7, 7), strides=(1, 1), activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(pool_size=(4, 4)),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ],
   "id": "6df476b11636184"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])",
   "id": "db98685b8e04ad91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TensorBoard\n",
    "log_file_name = f'brain_tumor_detection_cnn_{int(time.time())}'\n",
    "tensorboard = TensorBoard(log_dir=f'logs/{log_file_name}')"
   ],
   "id": "228e809c4604b7db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          batch_size=32, \n",
    "          epochs=20, \n",
    "          validation_data=(X_valid, y_valid), \n",
    "          callbacks=[tensorboard])\n"
   ],
   "id": "c6bc0b1d2e288ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "history = model.history.history\n",
   "id": "ef58391259cc9b4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualizing Training and Validation Metrics\n",
    "\n",
    "After training our Convolutional Neural Network (CNN), it’s essential to visualize the model’s performance over the training epochs. Plotting the training and validation metrics, such as accuracy and loss, helps us understand how well the model is learning and whether it’s overfitting or underfitting"
   ],
   "id": "e799688ca66f937d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Retrieve metrics from the history object\n",
    "train_loss = history.history['loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plotting Loss and Accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, 'b-o', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r-o', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_acc, 'b-o', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r-o', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "bba06df4307a6e57"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
